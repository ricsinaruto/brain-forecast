trf_class: SmoLLM3
hidden_size: 512
vocab_size: 8192
input_shape: [3200, 68, 4]  # T, H, W
spatial_reduction: [8, 1]
temporal_reduction: 8

tokenizer_path: /vol/data/trainings/omega/2sub/1to50hz_ss/brainomni_tok/logs/version_42/checkpoints/last-checkpoint-epoch00180.ckpt
# tokenizer_path: trainings/brainomni_tok/last-checkpoint-epoch00180.ckpt

trf_args:
  intermediate_size: 2048
  num_hidden_layers: 12  # 36 in 3B model
  num_attention_heads: 8
  num_key_value_heads: 2
  attention_dropout: 0.0
  
  max_position_embeddings: 16384  # should be higher than T' x H' x W'
  pad_token_id: null
  no_rope_layer_interval: 4
  rope_theta: 1000000.0

  _attn_implementation: flex_attention
  block_size: 4

  no_rope_layers: [
    1,
    1,
    1,
    0,
    1,
    1,
    1,
    0,
    1,
    1,
    1,
    0
  ]
