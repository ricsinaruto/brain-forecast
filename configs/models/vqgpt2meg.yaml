tokenizer_path: trainings/emu3visionvq/lightning_logs/version_23/checkpoints/epoch=99-step=100.ckpt

trf_args:
  vocab_size: 8192
  d_model: 64
  hidden_size: 64
  intermediate_size: 64
  layers: 8
  num_experts: 16
  experts_per_token: 2
  swiglu_limit: 7.0
  head_dim: 16
  num_attention_heads: 4
  num_key_value_heads: 4
  sliding_window: 0
  initial_context_length: 1024
  rope_theta: 150000.0
  rope_scaling_factor: 32.0
  rope_ntk_alpha: 1.0
  rope_ntk_beta: 32.0