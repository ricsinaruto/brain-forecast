model_config: configs/libribrain/bendr.yaml
augmentations: configs/libribrain/augs.yaml

save_dir: /vol/data/trainings/libribrain/bendr # trainings/cnnlstm 
resume_from: null

model_name: ClassifierContinuous
loss_name: CrossEntropy
dataset_name: libribrain

datasplitter:
  dataset_class: IndexedLabelGroupedDataset
  dataset_root: /vol/data/LibriBrain  # /Users/richard/Datasets/LibriBrain
  grouped_samples_std: 20  # fixed
  grouped_samples_mean: 80  # fixed

dataloader:
  batch_size: 64  # max for gpu memory
  num_workers: 16
  prefetch_factor: 2  # 2 is best
  pin_memory: true
  persistent_workers: true

loss:
  label_smoothing: 0.1
  l1: 0.0

lightning:
  lr: 5.0e-5  # reduce if loss is stuck
  weight_decay: 0.001
  compile: true
  log_samples: false
  log_psd: false
  sfreq: 200
  # Optional: configure a learning rate scheduler from torch.optim.lr_scheduler
  # lr_scheduler:
  #   class_name: CosineAnnealingLR   # any class from torch.optim.lr_scheduler
  #   T_max: 100                      # constructor kwargs for the scheduler
  #   eta_min: 1.0e-6
  #   interval: epoch                 # 'step' or 'epoch' (how often to step)
  #   frequency: 1                    # apply every N intervals
  #   monitor: val_loss               # required for ReduceLROnPlateau

trainer:
  max_epochs: 1000
  accelerator: cuda
  profiler: null
  fast_dev_run: false
  check_val_every_n_epoch: 1
  log_every_n_steps: 20
  precision: 16-mixed
