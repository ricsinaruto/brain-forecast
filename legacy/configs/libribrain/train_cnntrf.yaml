model_config: configs/libribrain/cnntrf_small.yaml
save_dir: /vol/data/trainings/libribrain/cnntrf # trainings/cnnlstm 
resume_from: # /vol/data/trainings/libribrain/cnntrf/lightning_logs/version_6/checkpoints/epoch=74-step=20850.ckpt

model_name: CNNLSTM
loss_name: CrossEntropy  # CrossEntropy
dataset_name: libribrain

datasplitter:
  dataset_class: GroupedDatasetAugmented
  dataset_root: /vol/data/LibriBrain  # /Users/richard/Datasets/LibriBrain
  grouped_samples_mean: 100  # 100
  augmentations: configs/libribrain/augs.yaml
  normalize: true  # false
  dataset_args:
    balanced: false

dataloader:
  batch_size: 64  # max for gpu memory
  num_workers: 16
  prefetch_factor: 4
  pin_memory: true
  persistent_workers: true

loss:
  label_smoothing: 0.05
  l1: 0.0
  use_f1: true

lightning:
  lr: 2.0e-4
  weight_decay: 0.01
  compile: true
  log_samples: false
  log_psd: false
  sfreq: 200
  # Optional: configure a learning rate scheduler from torch.optim.lr_scheduler
  lr_scheduler:
    class_name: CosineAnnealingLR
    eta_min: 2.0e-6
    T_max: 200

trainer:
  max_epochs: 200
  accelerator: cuda
  profiler: null
  fast_dev_run: false
  check_val_every_n_epoch: 1
  log_every_n_steps: 20
  precision: 16-mixed
  gradient_clip_val: 10.0
