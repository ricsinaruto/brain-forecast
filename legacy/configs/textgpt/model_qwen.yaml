trf_class: Qwen2_5_VideoText
tok_class: BPETokenizer
hidden_size: 768
vocab_size: 32768
input_shape: [3000, 68, 1]  # T, H, W

tok_args:
  tokenizer_path: /vol/data/datasets/Omega/preprocessed_1to50hz_ss_5bit_txt
  group_size: 5

trf_args:
  intermediate_size: 3072
  num_hidden_layers: 8
  num_attention_heads: 8
  num_key_value_heads: 2
  attention_dropout: 0.2
  
  max_position_embeddings: 150000  # should be higher than T' x H' x W'
  rope_theta: 5000000.0
  rope_scaling:  # can experiment with different types
    rope_type: default
    mrope_section: 12  # should be hidden_size / attention_heads // 6
