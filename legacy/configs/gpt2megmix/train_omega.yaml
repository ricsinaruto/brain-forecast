# Data loader parameters
# example_seconds and overlap_seconds refer to the raw signal duration for each example
# expressed in seconds

# Augmentations are optional and can be enabled by setting parameters

# Separate model configuration file
model_config: configs/gpt2megmix/model68.yaml
augmentations: configs/augmentations.yaml

save_dir: /vol/data/trainings/omega/1to50hz_ss/gpt2megmix
resume_from: null

model_name: GPT2MEGMix
loss_name: CrossEntropy

datasplitter:
  dataset_class: ChunkDataset
  dataset_root: "/vol/data/datasets/Omega/preprocessed1to50hz_ss_cloud"
  example_seconds: 50.0
  overlap_seconds: 5.0
  val_ratio: 0.1
  test_ratio: 0.1
  refresh_cache: false  # need to refresh if any datasplitter args change
  cache_dir: /vol/data/datasets/Omega/local_cache/preprocessed1to50hz_ss_50sec

dataloader:
  batch_size: 32  # 64
  num_workers: 16
  prefetch_factor: 4
  pin_memory: true
  persistent_workers: true

k_step_free_run:
  enabled: false
  # Warm-up and rollout lengths are measured in discrete timesteps (samples)
  warmup_range: [400, 600]
  rollout_range: [40, 80]
  sample_strategy: argmax  # or "sample"
  temperature: 1.0
  log_lengths: true

loss:
  label_smoothing: 0.0
  l1: 0.0

lightning:
  lr: 2.0e-4
  weight_decay: 0.01
  compile: true
  log_samples: false
  log_psd: False
  sfreq: 200
  lr_scheduler:
    class_name: CosineAnnealingLR
    eta_min: 2.0e-6
    T_max: 100

trainer:
  max_epochs: 100
  accelerator: cuda
  profiler: null
  fast_dev_run: false
  check_val_every_n_epoch: 1
  log_every_n_steps: 200
  precision: 16-mixed
  gradient_clip_val: 10.0
